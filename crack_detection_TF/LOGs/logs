日志文件记录：
这是使用照片来寻找断裂的方法，参照kaust_log/crack_detection(.)的代码

test01~test10 is in windows 文档/ML_log/crack_detection


test_01: 使用FCN，256*256，简单构型，只选了20张图片进行人工标注（./samples），然后batch_size=10,标注了很多的细小断裂，测试效果不佳。
test_02: 参数与test01基本相同，只是weight权重变化了点
由于样本太少，效果不好。

test_03：我使用了更多的图片，但是标注的更糙，接近20的标签宽度，然后主要选取的是宽大断裂（./samples_big）实验参数时batchsize=10, 20:100
test_04: 作弊，只使用./samples_big/00.jpg生成标签，其他参数同test03， 但是比重变成了1：2
test_05: 同test04, 但是尝试事先做归一化，使用test04的参数
test_06：05意外停止，所以我重新来一次

即使是在训练集上面，实验效果依然不好，test06基本已经出现了过拟合的现象。我的猜测是256*256的尺寸虽然够大，但是我的感受野只有50*50左右。
同时，crack的尺度会很长，而且个人感觉图片里面的像素很多事无用像素，反而导致相同尺寸下crack特征不明显。
所以我认为应该先降采样3倍，从肉眼观察来看，这个尺寸是非常适合肉眼的识别。因为明显地显示出了断裂的横向特征。

test_07:降采(1920, 2560).效果相比于test06有所提升，但是依然存在一些问题，无法实现generalization效果。

test_08:在test_07的基础上，准备只是用单通道灰度图，同时在batch 的时候提前归一化到-1~1
test_09:增大卷积核尺寸到5*5，基本无用

test_10：卷几何尺寸3*3,增加一层带pool的层


后续使用VGG16的前三个卷积块的参数，尝试提高通用性,放弃灰度图（无用）。后面的两个卷几块是5*5*256的尺寸，总体的感受野在128左右
预先归一化的方式为d/128-1

VGG/test_VGG01: 使用了VGG前三个模块，训练了5500ci,batchsize=1,样本只来自于./simples_big/00.jpg文件与标签。
下面的三个（VGG01~VGG03）是在超算上跑的(不带test)，训练参数都是size=10。一开始在图像的边缘存在点阵，不知道为啥
VGG/VGG01： 图片01~08.jpg 8张图片参与训练，00.jpg作为测试集
VGG/VGG01_1: 同vgg01，但是采用了weight=50
VGG/VGG02： 图片00.jpg采500个样，进行训练
VGG/VGG03： 图片00~08.jpg全部参与训练


由于上面的实验结果虽然拟合特征的能力很强，但是老师认为标注太不连续，希望能加入更多的特征，因而下面使用灰度图作为一个通道，
另外两个通道被设计为edgedetection 的结果（0/255），其他部分同上。
VGG/test_VGG03 4000次训练,01~08图片，单样本训练，训练结果说的过去，调整了一下edge detection 的数值，使之在归一化之后数值是0~1
VGG/VGG04 01~08，其他参数相同，weight=10.batchsize=10, 40k次（2400个样本）
VGG/VGG05 使用与VGG04相同的参数与训练数据，但是增大weight=50


上面的实验结果都不好，所以我尝试增加反卷积部分的层数(UNET)，从而使得结果更加精确。考虑到tf自带的交叉熵方法已经存在sigmoid,我们就不再对最后一层进行激活，只是进行BN操作（但是失败了，很容易不稳定）
test_UNET01: 逐层反卷积得到的结果，结果不错，但是部分垂直向的断层没有被识别出来。
test_UNET02: 为最后的两层卷积层添加dropout,同时在dataset上添加随机的旋转（90,180,270），(weight=50，没使用，因为样本在800的时候太粗了),增加样本集到3k。
			 注: dropout 层在这里是每一个样本随机丢掉一般的通道，但是不同样本间channel不一定相同，noiseshape=[NUM,1,1,c]
test_UNET03: 由于02的结果中，断层所占的比例太大，虽然标注的断层一般是存在小断层的，但是主次不分，很影响观感，所以使用先前对断层控制更加严格的部分作为训练。
			 同时dropout=[1,1,1,c], weight=10
test_UNET04:我逐渐感觉现在不再是generlization 的问题了，更多的问题存在与精度不够，细小断层的存在导致神经网络处处响应，然后被放大之后就显得很杂乱
			所以在这里将weight=1：GG800步的时候已经死掉了
						weight=5:也很容易GG
						说明希望通过调节weight的方法调节粗细已经不实用了。
						只能通过调整网络结构来实现，将第一层与第二层的卷积结果添加进去
test_UNET05:在最后一个范卷积层前面与后面添加了对应尺寸的卷积层的结果。结果weight=10很容易陷入局部极小，所以在提取样本的时候加了一个必须含有crack点的约束。
			依然遇见了局部极小的问题，结果发现在最后融合之后，卷积层需要两个才能不会陷入局部极小
